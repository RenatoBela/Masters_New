import os
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import configure

# Create the environment
env = make_vec_env("CarRacing-v2", n_envs=1)

# Create directories for logs and models
log_dir = "logs"
model_dir = "models"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

# Define a custom callback for logging additional metrics
class CustomTensorBoardCallback(BaseCallback):
    def __init__(self, log_dir, verbose=0):
        super(CustomTensorBoardCallback, self).__init__(verbose)
        self.log_dir = log_dir
        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        if self.locals.get('dones', None) is not None:
            for done, reward in zip(self.locals['dones'], self.locals['rewards']):
                if done:
                    # Log episode reward and length
                    episode_reward = sum(self.episode_rewards)
                    episode_length = len(self.episode_rewards)
                    self.logger.record('episode_reward', episode_reward)
                    self.logger.record('episode_length', episode_length)
                    self.episode_rewards = []
                    self.episode_lengths = []
                else:
                    self.episode_rewards.append(reward)
                    self.episode_lengths.append(1)
        return True

def run_model(model_name, model, log_subdir, num_runs=3):
    avg_reward = 0
    avg_steps = 0

    for run in range(1, num_runs + 1):
        run_log_dir = os.path.join(log_subdir, f"run_{run}")
        model.tensorboard_log = run_log_dir  # Set tensorboard log for each run
        callback = CustomTensorBoardCallback(run_log_dir)
        model.learn(total_timesteps=500000, callback=callback)

        # Save the model after each run
        model.save(os.path.join(model_dir, f"{model_name}_run_{run}"))

        # Testing the model
        obs = env.reset()
        total_rewards = 0
        steps = 0
        done = False
        while not done:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            total_rewards += reward
            steps += 1

        avg_reward += total_rewards
        avg_steps += steps

    # Log average results to TensorBoard
    avg_reward /= num_runs
    avg_steps /= num_runs
    avg_log_dir = os.path.join(log_subdir, "average")
    tb_writer = configure(avg_log_dir)
    tb_writer.record("average_reward", avg_reward)
    tb_writer.record("average_steps", avg_steps)
    tb_writer.dump()

# Initialize models
models = {
    #"model_baseline": PPO("CnnPolicy", env, verbose=1),
    #"model_1": PPO("CnnPolicy", env, verbose=1, n_steps=4096, batch_size=128),
    #"model_2": PPO("CnnPolicy", env, verbose=1, learning_rate=0.0001, clip_range=0.1),
    "model_3": PPO("CnnPolicy", env, verbose=1, gamma=0.995, gae_lambda=0.98),
    "model_4": PPO("CnnPolicy", env, verbose=1, ent_coef=0.01)
}

# Run each model three times and log results
for model_name, model in models.items():
    run_model(model_name, model, os.path.join(log_dir, model_name))

env.close()
